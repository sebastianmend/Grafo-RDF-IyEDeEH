
import nbformat as nbf
import os

nb = nbf.v4.new_notebook()

# Define the cells
cells = []

# --- Markdown Header ---
cells.append(nbf.v4.new_markdown_cell("""
# üåæ Enriquecimiento Sem√°ntico y Visualizaci√≥n (Explicado Paso a Paso)

Este notebook es una versi√≥n **comentada y educativa** del proceso de enriquecimiento de un Grafo de Conocimiento de Agricultura de Precisi√≥n.

**Objetivo:**
Conectar nuestro grafo local (generado a partir de papers) con la nube de datos enlazados (LOD), espec√≠ficamente **DBpedia**, para:
1.  **Identificar** entidades en los textos (Entity Linking).
2.  **Traer** informaci√≥n nueva sobre esas entidades (Enrichment).
3.  **Analizar** el resultado con consultas complejas y visualizaciones.

---
"""))

# --- Phase 0 ---
cells.append(nbf.v4.new_markdown_cell("""
## Fase 0: Configuraci√≥n y Carga

Aqu√≠ importamos las librer√≠as necesarias:
*   `rdflib`: Para manejar grafos RDF.
*   `SPARQLWrapper`: Para hacer consultas a DBpedia remotamente.
*   `pandas`, `matplotlib`, `seaborn`: Para manejo de datos y gr√°ficas.
*   `requests`: Para hablar con la API de DBpedia Spotlight.

Tambi√©n configuramos las rutas para que el notebook funcione tanto en la carpeta `notebooks/` como en la ra√≠z.
"""))

code_phase_0 = """
import pandas as pd  # Librer√≠a para manejo de DataFrames (tablas de datos similares a Excel)
import rdflib  # Librer√≠a principal de Python para trabajar con grafos RDF
from rdflib import Graph, Literal, URIRef, Namespace  # Clases b√°sicas de RDF: Grafo, Literal (dato), URI (identificador), Namespace (prefijo)
from rdflib.namespace import RDF, RDFS, OWL, FOAF, DCTERMS  # Namespaces est√°ndar predefinidos que usaremos
import requests  # Librer√≠a para hacer peticiones HTTP a APIs web (como la de DBpedia Spotlight)
import time  # Librer√≠a para controlar el tiempo (usaremos pausas para no saturar las APIs)
from SPARQLWrapper import SPARQLWrapper, JSON  # Cliente especializado para consultar endpoints SPARQL remotos
import matplotlib.pyplot as plt  # Librer√≠a base para crear gr√°ficos
import seaborn as sns  # Librer√≠a de gr√°ficos estad√≠sticos que mejora la est√©tica de matplotlib
import networkx as nx  # Librer√≠a para an√°lisis y visualizaci√≥n de grafos (redes) y nodos
import zipfile  # Librer√≠a para crear y manipular archivos ZIP comprimidos
import os  # Librer√≠a para interactuar con el sistema operativo (rutas, archivos, directorios)
from tqdm import tqdm  # Librer√≠a para mostrar barras de progreso en bucles largos

# --- Configuraci√≥n de Rutas Robustas ---
# Este bloque determina din√°micamente d√≥nde estamos ejecutando el script
# para construir las rutas a los archivos correctamente.
import os
if os.path.exists("notebooks"):
    # Si existe la carpeta "notebooks", asumimos que estamos en la ra√≠z del proyecto
    BASE_DIR = "notebooks"
elif os.path.exists("data") and os.path.exists("out"):
    # Si existen "data" y "out", asumimos que ya estamos DENTRO de "notebooks"
    BASE_DIR = "."
else:
    # Caso por defecto (fallback) si no reconocemos la estructura
    BASE_DIR = "."

# Construimos las rutas absolutas o relativas basadas en BASE_DIR
DATA_DIR = os.path.join(BASE_DIR, "data", "processed")  # Carpeta donde est√°n los datos procesados (CSVs)
OUT_DIR = os.path.join(BASE_DIR, "out")  # Carpeta de salida para guardar el grafo y las im√°genes

# Definimos rutas espec√≠ficas a archivos clave que usaremos
INPUT_TTL = os.path.join(OUT_DIR, "agri_graph_improved.ttl")  # Ruta del grafo RDF generado previamente
OUTPUT_TTL = os.path.join(OUT_DIR, "enriched_graph.ttl")  # Ruta donde guardaremos el grafo enriquecido final
PAPERS_CSV = os.path.join(DATA_DIR, "papers.csv")  # Ruta del CSV con metadatos de los papers
FIELDS_CSV = os.path.join(DATA_DIR, "fields.csv")  # Ruta del CSV con las palabras clave (keywords)

# --- Definici√≥n de Namespaces ---
# EX: Nuestro namespace personalizado para este proyecto de agricultura
EX = Namespace("http://example.org/agri#")
# DBO y DBR: Namespaces oficiales de DBpedia (Ontolog√≠a y Recursos)
DBO = Namespace("http://dbpedia.org/ontology/")
DBR = Namespace("http://dbpedia.org/resource/")

# --- Carga del Grafo Inicial ---
print(f"Cargando grafo base desde: {os.path.abspath(INPUT_TTL)}")
# Verificamos que el archivo exista antes de intentar cargarlo para evitar errores feos
if not os.path.exists(INPUT_TTL):
    raise FileNotFoundError(f"No se encuentra el archivo: {INPUT_TTL}")

g = Graph()  # Instanciamos un nuevo objeto grafo vac√≠o de RDFLib
g.parse(INPUT_TTL, format="turtle")  # Leemos y parseamos el archivo Turtle (.ttl) en memoria
print(f"Grafo inicial cargado: {len(g)} triples")  # Imprimimos la cantidad de hechos (triples) cargados

# --- Carga de Metadatos (CSVs) ---
print(f"Cargando metadatos para entity linking...")
papers_df = pd.read_csv(PAPERS_CSV)  # Cargamos el archivo papers.csv en un DataFrame de Pandas
fields_df = pd.read_csv(FIELDS_CSV)  # Cargamos el archivo fields.csv en un DataFrame de Pandas

# --- Limpieza de Datos ---
# Eliminamos filas que no tengan abstract o ID, ya que son in√∫tiles para el an√°lisis sem√°ntico
papers_df = papers_df.dropna(subset=['abstract', 'paperId'])
# Eliminamos filas donde el abstract sea solo espacios en blanco (vac√≠o visualmente)
papers_df = papers_df[papers_df['abstract'].str.strip() != ""]
print(f"Papers listos para procesar: {len(papers_df)}")
"""
cells.append(nbf.v4.new_code_cell(code_phase_0))

# --- Phase A: Entity Linking ---
code_phase_a = """
print("--- Iniciando Fase A: Entity Linking (DBpedia Spotlight) ---")

# URL de la API p√∫blica de DBpedia Spotlight (servicio de anotaci√≥n sem√°ntica)
SPOTLIGHT_API = "https://api.dbpedia-spotlight.org/en/annotate"
CONFIDENCE = 0.5  # Umbral de confianza: solo aceptamos entidades con score > 0.5 (falsos positivos vs negativos)
SUBSET_LIMIT = 50 # PROCESAMIENTO PARCIAL: Solo procesamos los primeros 50 papers para demostraci√≥n r√°pida y no bloquear la API

# --- Paso 1: Enlazar Texto de Papers con Entidades ---
count = 0
# iterrows() nos permite recorrer el DataFrame fila por fila.
# tqdm() envuelve el iterador para mostrarnos una barra de progreso visual en la consola/notebook.
for index, row in tqdm(papers_df.head(SUBSET_LIMIT).iterrows(), total=SUBSET_LIMIT, desc="Annotating Papers"):
    abstract = row['abstract']  # Extraemos el texto del abstract para analizar
    paper_id = row['paperId']   # Extraemos el ID del paper para construir su URI
    
    # Reconstruimos la URI del paper tal como existe en nuestro grafo RDF usando f-string.
    # Debe coincidir exactamente con el patr√≥n usado en generate_rdf.py (ej: http://example.org/agri#art-123)
    paper_uri = URIRef(f"{EX}art-{str(paper_id)}")
    
    try:
        # Preparamos la petici√≥n HTTP.
        # Headers: Indicamos que queremos la respuesta en formato JSON (estructurada).
        headers = {'Accept': 'application/json'}
        # Data: Los par√°metros obligatorios que espera la API (el texto y la confianza m√≠nima).
        data = {'text': abstract, 'confidence': CONFIDENCE}
        
        # Realizamos una petici√≥n POST. Usamos POST en lugar de GET porque los abstracts 
        # pueden ser muy largos y exceder el l√≠mite de caracteres de una URL en una petici√≥n GET.
        response = requests.post(SPOTLIGHT_API, data=data, headers=headers, timeout=15)
        
        # Si la respuesta es exitosa (c√≥digo HTTP 200 OK)
        if response.status_code == 200:
            res_json = response.json()  # Parseamos el JSON recibido a un diccionario Python
            
            # Verificamos si la API encontr√≥ 'Resources' (la lista de entidades detectadas)
            if 'Resources' in res_json:
                for res in res_json['Resources']:
                    # Extraemos la URI de DBpedia de la entidad encontrada (ej: http://dbpedia.org/resource/Machine_learning)
                    dbpedia_uri = URIRef(res['@URI'])
                    
                    # Creamos un nuevo triple RDF:
                    # Sujeto: Nuestro paper local (paper_uri)
                    # Predicado: schema:mentions ("menciona")
                    # Objeto: La entidad de DBpedia descubierta (dbpedia_uri)
                    g.add((paper_uri, URIRef("http://schema.org/mentions"), dbpedia_uri))
        
        # IMPORTANTE: Pausa de 0.5 segundos entre peticiones para no ser bloqueados por la API p√∫blica (Rate Limiting)
        time.sleep(0.5)
        
    except Exception as e:
        # Capturamos cualquier error de red o parsing y seguimos con el siguiente paper sin detener el script
        print(f"Error procesando paper {paper_id}: {e}")

# --- Paso 2: Enlazar Keywords con Entidades (owl:sameAs) ---
print("\\nEnlazando Keywords (Fields)...")
# Iteramos sobre todas las keywords disponibles en fields_df
for index, row in tqdm(fields_df.iterrows(), total=len(fields_df), desc="Linking Fields"):
    field_name = row['fieldName']  # La keyword (ej: "Artificial Intelligence")
    
    try:
        # Para keywords cortas usamos GET, es m√°s r√°pido y el est√°ndar para consultas simples.
        params = {'text': field_name, 'confidence': CONFIDENCE}
        headers = {'Accept': 'application/json'}
        response = requests.get(SPOTLIGHT_API, params=params, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if 'Resources' in data:
                # Tomamos la primera coincidencia (la m√°s probable seg√∫n DBpedia)
                dbpedia_uri = URIRef(data['Resources'][0]['@URI'])
                
                # Buscamos en nuestro grafo local el nodo Concepto (skos:Concept) 
                # que tenga esta etiqueta exacta (field_name).
                query = f\"\"\"
                SELECT ?c WHERE {{
                    ?c a <http://www.w3.org/2004/02/skos/core#Concept> .
                    ?c <http://www.w3.org/2004/02/skos/core#prefLabel> "{field_name}" .
                }}
                \"\"\"
                res = g.query(query) # Ejecutamos la b√∫squeda local
                
                # Si encontramos el concepto local, le a√±adimos un link 'owl:sameAs'
                # Esto significa: "Nuestro concepto local X es sem√°nticamente id√©ntico al recurso Y de DBpedia"
                for r in res:
                    g.add((r.c, OWL.sameAs, dbpedia_uri))
        
        time.sleep(0.2) # Pausa breve de cortes√≠a
    except Exception as e:
        print(f"Error linking field {field_name}: {e}")

# Mostramos el tama√±o del grafo tras a√±adir todos los nuevos enlaces
print(f"Grafo despu√©s del entity linking: {len(g)} triples")
"""
cells.append(nbf.v4.new_code_cell(code_phase_a))

# --- Phase B: Enrichment ---
code_phase_b = """
print("--- Iniciando Fase B: Enriquecimiento Sem√°ntico (SPARQL Federation) ---")

# --- Paso 1: Recolectar todas las URIs externas ---
dbpedia_uris = set() # Usamos un set (conjunto) para evitar duplicados autom√°ticamente
# Buscamos todas las URIs que son objeto de 'schema:mentions' y pertenecen al dominio dbpedia.org
for o in g.objects(None, URIRef("http://schema.org/mentions")):
    if "dbpedia.org/resource" in str(o):
        dbpedia_uris.add(o)
# Buscamos todas las URIs que son objeto de 'owl:sameAs' y pertenecen al dominio dbpedia.org
for o in g.objects(None, OWL.sameAs):
    if "dbpedia.org/resource" in str(o):
        dbpedia_uris.add(o)

print(f"Total de entidades √∫nicas a enriquecer encontradas: {len(dbpedia_uris)}")

# --- Paso 2: Configurar Cliente SPARQL ---
# Inicializamos el wrapper apuntando al endpoint oficial y p√∫blico de DBpedia
sparql = SPARQLWrapper("http://dbpedia.org/sparql")
uris_list = list(dbpedia_uris) # Convertimos el set a lista para poder indexar y dividir
BATCH_SIZE = 50 # Consultaremos de 50 en 50 para no hacer una query gigante que falle por timeout

# --- Paso 3: Iterar y Consultar ---
# range(start, stop, step) nos permite saltar de 50 en 50 √≠ndices
for i in tqdm(range(0, len(uris_list), BATCH_SIZE), desc="Enriching Batches"):
    batch = uris_list[i:i+BATCH_SIZE] # Seleccionamos el subgrupo (lote) actual de URIs
    
    # Construimos la cl√°usula VALUES de SPARQL din√°micamente. 
    # Esto inyecta las 50 URIs directamente en la consulta como una tabla temporal.
    # Formato resultante: <uri1> <uri2> ...
    values_clause = " ".join([f"<{u}>" for u in batch])
    
    # Construimos la consulta SPARQL Federada
    # Pedimos: Nombre (foaf:name), Tema (dct:subject) y Resumen (dbo:abstract)
    # Usamos OPTIONAL para traer lo que exista, sin fallar si a una entidad le falta alg√∫n dato.
    query = f\"\"\"
    PREFIX foaf: <http://xmlns.com/foaf/0.1/>
    PREFIX dct: <http://purl.org/dc/terms/>
    PREFIX dbo: <http://dbpedia.org/ontology/>
    
    SELECT ?res ?name ?subject ?abstract WHERE {{
        VALUES ?res {{ {values_clause} }}
        OPTIONAL {{ ?res foaf:name ?name . }}
        OPTIONAL {{ ?res dct:subject ?subject . }}
        OPTIONAL {{ ?res dbo:abstract ?abstract . FILTER(LANG(?abstract) = "en") }} # Solo abstracts en ingl√©s
    }}
    \"\"\"
    
    try:
        # Configuramos y ejecutamos la query remota
        sparql.setQuery(query)
        sparql.setReturnFormat(JSON) # Queremos JSON estructurado de vuelta
        results = sparql.query().convert() # Ejecutar y convertir a diccionario Python
        
        # Procesamos cada resultado devuelto por DBpedia
        for result in results["results"]["bindings"]:
            res_uri = URIRef(result["res"]["value"]) # La URI de la entidad (el sujeto)
            
            # Si vino el nombre, lo agregamos a nuestro grafo local con predicado foaf:name
            if "name" in result:
                g.add((res_uri, FOAF.name, Literal(result["name"]["value"])))
            
            # Si vino el tema (subject), lo agregamos como URI con predicado dct:subject
            if "subject" in result:
                g.add((res_uri, DCTERMS.subject, URIRef(result["subject"]["value"])))
            
            # Si vino el abstract, lo agregamos como Literal con idioma ingl√©s
            if "abstract" in result:
                g.add((res_uri, DBO.abstract, Literal(result["abstract"]["value"], lang='en')))
                
    except Exception as e:
        # Si falla un lote (ej: timeout de DBpedia), lo reportamos y continuamos con el siguiente
        print(f"Error en el lote SPARQL (√≠ndice {i}): {e}")
        time.sleep(1) # Pausa de seguridad

# --- Paso 4: Guardar Resultado ---
# Serializamos el grafo completo (original + enriquecimientos) a disco en formato Turtle
g.serialize(destination=OUTPUT_TTL, format="turtle")
print(f"‚úÖ Grafo Enriquecido guardado exitosamente en: {OUTPUT_TTL}")
"""
cells.append(nbf.v4.new_code_cell(code_phase_b))

# --- Phase C: Queries ---
code_phase_c = """
print("--- Fase C: Ejecutando Consultas SPARQL Anal√≠ticas ---")

# --- Consulta 1: Top Entidades Mencionadas ---
# Objetivo: Contar cu√°ntas veces aparece cada URI mencionada en 'schema:mentions'
# Agrupamos por ?entity, contamos ?paper, ordenamos descendente y limitamos a 10.
q1 = \"\"\"
PREFIX schema: <http://schema.org/>
SELECT ?entity (COUNT(?paper) as ?mentionCount) WHERE {
    ?paper schema:mentions ?entity .
} GROUP BY ?entity ORDER BY DESC(?mentionCount) LIMIT 10
\"\"\"
print("\\n1. Top 10 Entidades Mencionadas (Output de consola):")
# Ejecutamos la query contra el grafo 'g'
for row in g.query(q1):
    # row.entity es la URI completa, usamos split('/')[-1] para ver solo el nombre final por limpieza
    print(f"{row.mentionCount} menciones - {row.entity.split('/')[-1]}")

# --- Consulta 2: Papers de Alto Impacto con T√≥picos Enriquecidos ---
# Objetivo: Ver qu√© temas tratan los papers m√°s citados.
# Usamos 'COALESCE' para robustez: si la entidad tiene nombre (tra√≠do de DBpedia), lo usa.
# Si no, usa la URI convertida a string. Esto evita celdas vac√≠as si el enriquecimiento fall√≥ parcialmente.
q2 = \"\"\"
PREFIX schema: <http://schema.org/>
PREFIX ex: <http://example.org/agri#>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?title ?cites ?entityName WHERE {
    ?paper a schema:Article ;          # Es una instancia de Article
           schema:title ?title ;       # Tiene un t√≠tulo
           ex:citationCount ?cites ;   # Tiene conteo de citas
           schema:mentions ?entity .   # Menciona una entidad
    
    # Intentamos obtener el nombre enriquecido (OPTIONAL)
    OPTIONAL { ?entity foaf:name ?n }
    
    # Elegimos ?n si existe, sino STR(?entity) (URI como string)
    BIND(COALESCE(?n, STR(?entity)) AS ?entityName)
    
    FILTER(?cites > 0) # Solo mostramos papers que tengan al menos 1 cita
} ORDER BY DESC(?cites) LIMIT 5
\"\"\"
print("\\n2. Papers de Impacto y sus Temas (Output de consola):")
for row in g.query(q2):
    print(f"[{row.cites}] {row.title[:40]}... -> {row.entityName}")

# --- Consulta 3: Red de Co-ocurrencia ---
# Objetivo: Encontrar pares de entidades que aparecen juntas en el mismo paper.
# Esto revela relaciones latentes entre conceptos (ej: si A y B aparecen mucho juntos, est√°n relacionados).
q3 = \"\"\"
PREFIX schema: <http://schema.org/>
SELECT ?e1 ?e2 (COUNT(?paper) as ?coCount) WHERE {
    ?paper schema:mentions ?e1 . # El paper P menciona entidad A (e1)
    ?paper schema:mentions ?e2 . # El MISMO paper P menciona entidad B (e2)
    
    # Filtro < evita que tengamos el par (A, B) y (B, A) duplicados y evita (A, A) consigo mismo
    FILTER(STR(?e1) < STR(?e2)) 
} GROUP BY ?e1 ?e2 ORDER BY DESC(?coCount) LIMIT 15
\"\"\"
print("\\n3. Top Pares Co-ocurrentes (Output de consola):")
for row in g.query(q3):
    print(f"{row.coCount}: {row.e1.split('/')[-1]} <-> {row.e2.split('/')[-1]}")

# --- Consulta 4: Autores Pol√≠matas (Diversidad) ---
# Objetivo: Ranking de autores por cantidad de temas DIFERENTES (DISTINCT) sobre los que escriben.
# Usa schema:author para llegar al autor y schema:name para su nombre.
q4 = \"\"\"
PREFIX schema: <http://schema.org/>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?authorName (COUNT(DISTINCT ?entity) as ?topicCount) WHERE {
    ?paper schema:author ?author .      # Paper tiene autor
    ?author schema:name ?authorName .   # Autor tiene nombre
    ?paper schema:mentions ?entity .    # Paper menciona tema
} GROUP BY ?authorName ORDER BY DESC(?topicCount) LIMIT 10
\"\"\"
print("\\n4. Diversidad de Temas por Autor (Output de consola):")
for row in g.query(q4):
    print(f"{row.topicCount} temas √∫nicos - {row.authorName}")

# --- Consulta 5: Validaci√≥n de Keywords ---
# Objetivo: Verificar visualmente que los keywords locales (Concept) se conectaron a DBpedia correctamente.
q5 = \"\"\"
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
SELECT ?label ?dbpediaURI WHERE {
    ?c a skos:Concept ;
       skos:prefLabel ?label ; # Etiqueta local (ej: "Agriculture")
       owl:sameAs ?dbpediaURI . # URI remota enlazada (ej: dbpedia:Agriculture)
} LIMIT 5
\"\"\"
print("\\n5. Ejemplo de Mapeo Keywords -> DBpedia (Output de consola):")
for row in g.query(q5):
    print(f"{row.label} -> {row.dbpediaURI}")
"""
cells.append(nbf.v4.new_code_cell(code_phase_c))

# --- Phase D: Viz ---
code_phase_d = """
print("--- Fase D: Generando Visualizaciones Gr√°ficas ---")
# Establecemos el estilo visual de seaborn (fondo blanco con grilla, m√°s limpio)
sns.set_theme(style="whitegrid")

# --- VIZ 1: Gr√°fico de Barras (Top Entidades) ---
# Reutilizamos la Query 1 (q1) ya ejecutada en memoria (o ejecutamos de nuevo si queremos estar seguros)
res = g.query(q1)
data = []
# Procesamos los resultados de SPARQL a una lista de diccionarios Python
for row in res:
    data.append({
        "Entity": str(row.entity).split('/')[-1], # Limpiamos la URI para tener solo el nombre
        "Count": int(row.mentionCount)            # Convertimos el literal RDF a entero Python
    })
# Creamos un DataFrame de Pandas (formato tabular ideal para Seaborn)
df_viz = pd.DataFrame(data)

# Configuraci√≥n del gr√°fico
plt.figure(figsize=(10, 6)) # Definimos el tama√±o de la figura en pulgadas
sns.barplot(data=df_viz, x="Count", y="Entity", palette="viridis") # Barplot horizontal
plt.title("Top Entidades Mentionadas en Papers") # T√≠tulo
plt.xlabel("N√∫mero de Menciones") # Etiqueta eje X
plt.tight_layout() # Ajuste autom√°tico de m√°rgenes para que no se corten etiquetas
plt.savefig(os.path.join(OUT_DIR, "viz_top_entities.png")) # Guardar a archivo PNG
plt.show() # Mostrar en celda del notebook

# --- VIZ 2: Gr√°fico de Pastel (Top Categor√≠as/Subjects) ---
# Hacemos una nueva query para contar por dct:subject
q_sub = \"\"\"
PREFIX dct: <http://purl.org/dc/terms/>
SELECT ?subject (COUNT(?s) as ?subjCount) WHERE {
    ?s dct:subject ?subject .
} GROUP BY ?subject ORDER BY DESC(?subjCount) LIMIT 8
\"\"\"
res_sub = g.query(q_sub)
# List Comprehension para extraer etiquetas y valores limpiamente
labels = [str(r.subject).split(':')[-1] for r in res_sub]
sizes = [int(r.subjCount) for r in res_sub]

if sizes: # Verificamos que haya datos antes de graficar
    plt.figure(figsize=(8, 8)) # Gr√°fico cuadrado para el pastel
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140) # autopct muestra porcentajes
    plt.title("Distribuci√≥n de Categor√≠as (Subjects de DBpedia)")
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, "viz_subjects_pie.png"))
    plt.show()

# --- VIZ 3: Scatter Plot (Citas vs Menciones) ---
# Queremos ver si existe correlaci√≥n: ¬øpapers con m√°s menciones a entidades reciben m√°s citas?
q_scatter = \"\"\"
PREFIX schema: <http://schema.org/>
PREFIX ex: <http://example.org/agri#>
SELECT ?paper (SAMPLE(?cites) as ?citationCount) (COUNT(?entity) as ?mentionCount) WHERE {
    ?paper a schema:Article ;
           ex:citationCount ?cites ;
           schema:mentions ?entity .
} GROUP BY ?paper LIMIT 100
\"\"\"
res_scatter = g.query(q_scatter)
# CR√çTICO: Convertimos citationCount a float() porque puede venir como decimal RDF ("10.0")
# y el constructor int() de Python fallar√≠a con strings decimales.
data_scatter = [
    {"Cites": float(r.citationCount), "Mentions": int(r.mentionCount)} 
    for r in res_scatter
]
df_scatter = pd.DataFrame(data_scatter)

plt.figure(figsize=(8, 6))
# Usamos scatterplot para puntos XY
sns.scatterplot(data=df_scatter, x="Mentions", y="Cites", color="blue", alpha=0.6)
plt.title("Correlaci√≥n: Riqueza Sem√°ntica (Entidades) vs Impacto (Citas)")
plt.xlabel("Cantidad de Entidades Mencionadas")
plt.ylabel("Citas Recibidas")
plt.tight_layout()
plt.savefig(os.path.join(OUT_DIR, "viz_scatter.png"))
plt.show()

# --- VIZ 4: Grafo de Red (Co-ocurrencia) ---
# Usamos NetworkX para visualizar la red de conceptos
res_q3 = g.query(q3) # Reutilizamos Q3 (pares e1, e2, count)
G = nx.Graph() # Grafo vac√≠o de NetworkX

for row in res_q3:
    e1 = str(row.e1).split('/')[-1]
    e2 = str(row.e2).split('/')[-1]
    w = int(row.coCount) # Peso de la arista = frecuencia conjunta
    G.add_edge(e1, e2, weight=w) # A√±adimos arista entre e1 y e2 con peso w

plt.figure(figsize=(10, 8))
pos = nx.spring_layout(G, k=0.5) # Algoritmo de distribuci√≥n de nodos (fuerza, k regula separaci√≥n)
# Dibujamos nodos y aristas
nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=1500, font_size=10, edge_color='gray')
# Dibujamos las etiquetas de peso (n√∫meros) sobre las aristas
labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)
plt.title("Mapa de Conocimiento: Co-ocurrencia de Entidades")
plt.savefig(os.path.join(OUT_DIR, "viz_network.png"))
plt.show()

# --- VIZ 5: Barras Horizontal (Autores) ---
res_q4 = g.query(q4) # Reutilizamos Q4 (Autor, TopicCount)
data_auth = [{"Author": str(r.authorName), "Topics": int(r.topicCount)} for r in res_q4]
df_auth = pd.DataFrame(data_auth)

plt.figure(figsize=(10, 6))
# Gr√°fico horizontal (x=Topics, y=Author) facilita leer nombres largos de autores
sns.barplot(data=df_auth, x="Topics", y="Author", palette="magma")
plt.title("Ranking de Autores por Diversidad Tem√°tica")
plt.xlabel("Temas √önicos Tratados")
plt.tight_layout()
plt.savefig(os.path.join(OUT_DIR, "viz_authors.png"))
plt.show()
"""
cells.append(nbf.v4.new_code_cell(code_phase_d))

# --- Phase E: Zip ---
code_phase_e = """
print("--- Fase E: Empaquetado Final del Entregable ---")

# Lista de todos los archivos que queremos incluir en el ZIP final
files_to_zip = [
    PAPERS_CSV, # Datos crudos: papers
    FIELDS_CSV, # Datos crudos: keywords
    INPUT_TTL,  # Grafo original (sin enriquecer)
    OUTPUT_TTL, # Grafo enriquecido (Resultado final de fase B)
    os.path.join(OUT_DIR, "viz_top_entities.png"), # Imagen 1
    os.path.join(OUT_DIR, "viz_subjects_pie.png"), # Imagen 2
    os.path.join(OUT_DIR, "viz_scatter.png"),      # Imagen 3
    os.path.join(OUT_DIR, "viz_network.png"),      # Imagen 4
    os.path.join(OUT_DIR, "viz_authors.png"),      # Imagen 5
    "Enrichment_and_Viz_commented.ipynb"           # IMPORTANTE: Incluimos este mismo notebook
]

ZIP_NAME = "Entregable_Final.zip"
zip_path = os.path.join(OUT_DIR, ZIP_NAME)

# Usamos la librer√≠a zipfile para escribir el archivo comprimido
with zipfile.ZipFile(zip_path, 'w') as zf:
    for f in files_to_zip:
        if os.path.exists(f):
            # Escribimos el archivo en el zip, usando solo su nombre base 
            # (arcname=os.path.basename(f)) para evitar crear carpetas anidadas dentro del zip
            zf.write(f, os.path.basename(f))
        else:
            print(f"‚ö†Ô∏è Aviso: Archivo {f} no encontrado, se omitir√° del ZIP.")

print(f"‚úÖ ZIP Final Generado Exitosamente: {zip_path}")
"""
cells.append(nbf.v4.new_code_cell(code_phase_e))

# Assign cells
nb['cells'] = cells

# Save
OUTPUT_NB = r"notebooks/Enrichment_and_Viz_commented.ipynb"
with open(OUTPUT_NB, 'w', encoding='utf-8') as f:
    nbf.write(nb, f)

print(f"Notebook comentado creado en {OUTPUT_NB}")
